\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{float}
\usepackage{setspace}

\geometry{margin=1in}
\doublespacing

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=blue,
    urlcolor=blue
}

\title{Simulated Geometry: Why Information Alone Cannot Be Alive}

\author{Ian Todd\\
Sydney Medical School, University of Sydney\\
Sydney, NSW, Australia\\
\texttt{itod2305@uni.sydney.edu.au}}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
Large language models pass examinations and solve complex problems, yet something appears missing. We propose a precise alternative to contested philosophical diagnoses: \textbf{life instantiates geometry; AI simulates geometry using information}. Here, \emph{information} is what can be copied without loss (bits, facts), while \emph{geometry} is the constrained manifold within which a system operates---the dimensionality of knowing, not the quantity of known. Current AI has vast information but minimal self-maintained geometry: LLMs simulate relational structure using statistical patterns but do not instantiate it, as there is no underlying manifold whose dynamics are being maintained. We formalize this distinction using dynamical systems theory, defining \emph{match error} $\varepsilon$ as agreement between independent complexity estimators (geometric and spectral). We predict that living systems robustly minimize $\varepsilon$ while artificial systems show systematic mismatch. Biology provides the existence proof: life instantiates geometry through bidirectional cross-scale coupling, dormancy shows geometry can persist without dynamics, and death is geometric collapse. The implication for AI is architectural: instantiating geometry requires not more parameters but intrinsic temporal dynamics coupled to viability. The metabolic expense is not a cost of intelligence---it \emph{is} the intelligence.
\end{abstract}

\noindent\textbf{Keywords:} information, geometry, dimension matching, artificial intelligence, biological complexity, cross-scale coherence

%==============================================================================
\section{The Simulation Problem}
\label{sec:intro}
%==============================================================================

Something strange happened in 2022. Large language models began passing professional examinations, writing competent code, and conducting conversations indistinguishable from human dialogue. The Turing test, once considered a meaningful threshold, became trivially passable. Yet most observers---including many AI researchers---felt that something was still missing. The systems were impressive but not, in some hard-to-articulate sense, \emph{intelligent} the way living things are intelligent.

The standard responses to this intuition fall into familiar philosophical camps. Consciousness theorists argue that LLMs lack subjective experience. Embodiment advocates claim they need bodies. Grounding skeptics insist their symbols float free of meaning. Alignment researchers worry they are intelligent but misaligned. Each diagnosis points to something real, but none provides a precise, measurable criterion for what distinguishes the simulation from the genuine article.

We propose a different answer, grounded in mathematics rather than philosophy: \textbf{life instantiates geometry; AI simulates geometry using information}.

This claim requires unpacking. By \emph{information}, we mean what Shannon formalized: quantities that can be named, stored, transmitted, copied. Bits. Facts. Patterns in data. A database has information. A library has information. An LLM with trillions of parameters has vast information.

By \emph{geometry}, we mean something different: the shape of the constrained manifold within which a system operates. Not the facts it knows, but \emph{how those facts relate to each other and to action}. The dimensionality of its knowing, not just the quantity of its knowledge. A living cell has geometry. Its molecular reactions, membrane dynamics, and genetic regulation form a high-dimensional attractor that the cell actively maintains through continuous energy expenditure.

The key insight is that LLMs \emph{simulate} geometry using information. They learn statistical patterns that approximate the relational structure of concepts, and these approximations are often remarkably good. When you ask an LLM to reason, it traverses a learned geometry of relationships---but this geometry exists only as patterns in weights, not as a maintained dynamical manifold. There is no underlying structure whose coherence the system works to preserve. The simulation floats free.

Living systems are different. They \emph{instantiate} geometry through bidirectional cross-scale coupling: molecular dynamics constrain cellular behavior, and cellular behavior maintains molecular organization. This vertical coupling creates a genuine manifold---a constrained region of state space that the system actively occupies and defends. Perturbations are absorbed, damage is repaired, coherence is maintained. The geometry is real in a way that a simulation is not.

This distinction is not merely philosophical. It is measurable. We will show that systems with genuine geometry exhibit \emph{estimator agreement}: independent ways of measuring complexity yield consistent answers. Systems that simulate geometry show \emph{estimator mismatch}: high complexity by some measures, low by others. The signature of instantiated geometry is coherence across measurement modalities. The signature of simulated geometry is systematic inconsistency.

The paper proceeds as follows. Section~\ref{sec:distinction} develops the information/geometry distinction in detail. Section~\ref{sec:measurement} presents the mathematical framework for measuring geometry. Section~\ref{sec:biology} reviews evidence that living systems instantiate geometry. Section~\ref{sec:ai} analyzes why current AI does not. Section~\ref{sec:requirements} asks what would be required for AI to instantiate geometry. Section~\ref{sec:testing} proposes empirical tests. Section~\ref{sec:discussion} discusses implications.

%==============================================================================
\section{Information versus Geometry}
\label{sec:distinction}
%==============================================================================

\subsection{What Information Is}

Shannon's information theory \citep{shannon1948} provides a precise definition of information: the reduction of uncertainty. A message carries information to the extent that it narrows the space of possibilities. This is measured in bits---the logarithm of the number of distinguishable states.

Information in this sense is substrate-independent. The same information can be encoded in electrical signals, magnetic domains, ink on paper, or neural firing patterns. It can be copied perfectly, transmitted without loss (given sufficient redundancy), and stored indefinitely. Information is what survives translation between media.

This is information's great strength---and its limitation. Precisely because information is substrate-independent, it tells you nothing about the substrate. A complete description of a cell's information content (its genome, proteome, metabolome, the positions of all its molecules) would be astronomically large, but it would not tell you that the cell is alive. The same information could describe a dead cell, or a perfect simulation, or a frozen cell that will never resume function.

\subsection{What Geometry Is}

By \emph{geometry}, we mean the shape of the dynamical manifold within which a system operates. This is not the same as the information content of the system's state. It is the structure of the \emph{constraints} that limit what states are reachable and what transitions are possible.

Consider a simple example. A pendulum swinging in a plane traces out trajectories on a 2D manifold (position and velocity). A double pendulum traces trajectories on a 4D manifold. A human heart, with its ion channels, contractile proteins, and pacemaker cells, operates on a manifold of vastly higher dimension---but not arbitrary dimension. The heart's geometry is constrained by anatomy, physiology, and the active regulation that maintains cardiac function.

The geometry of a system is revealed not by counting its degrees of freedom, but by measuring how its dynamics are organized. Two systems can have the same number of components but radically different geometry. A bag of heart cells has the same molecular inventory as a working heart, but its geometry has collapsed: there is no coordinated manifold, only independent decay toward equilibrium.

\subsection{The Distinction Made Precise}

The crucial difference:
\begin{itemize}
    \item \textbf{Information} is what can be copied without loss. It is substrate-independent.
    \item \textbf{Geometry} cannot be copied as information alone; it must be re-instantiated as a maintained constraint architecture. It is substrate-dependent.
\end{itemize}

You can copy a cell's genome (information). You cannot copy the cell's dynamical coherence (geometry) by copying data alone---you would have to \emph{rebuild} it, constructing a substrate capable of realizing and maintaining the same constraint architecture. Information can specify geometry, but only machines can instantiate it.

This is why the name of a thing is information, but what the thing \emph{is}---how it functions, persists, responds---is geometry. The name ``heart'' is information. The actual heart is a geometric structure: a manifold of possible states, actively maintained against entropy, defended against perturbation.

\subsection{LLMs Simulate Geometry}

Large language models are trained on text---information. They learn statistical patterns that capture, to a remarkable degree, the relational structure of concepts. When prompted, they can traverse these learned relationships in ways that approximate reasoning.

But the geometry they traverse exists only as patterns in weights. There is no underlying manifold being maintained. No oscillations stabilize the structure. No viability constraints enforce coherence. The ``geometry'' is a map, not a territory.

This is not a criticism of LLMs' capabilities. Maps can be extraordinarily useful. A good map lets you navigate effectively without ever touching the territory. LLMs are, in this sense, the best maps ever created---maps so detailed that they can simulate the experience of navigating the territory.

But a map is not alive. It does not maintain itself. It does not repair damage. It does not care whether it persists. The simulation of geometry is not the instantiation of geometry.

\subsection{Life Instantiates Geometry}

Living systems are different. They do not merely represent geometric relationships---they \emph{are} geometric relationships, instantiated in matter and maintained by dynamics.

A living cell occupies a constrained region of state space. Its molecular concentrations, membrane potentials, and protein conformations are not arbitrary---they are held within viable ranges by active regulation. When perturbed, the cell responds to restore homeostasis. When damaged, it repairs or adapts. The geometry is not stored as information but enacted as ongoing process.

This instantiated geometry has a measurable signature: \emph{cross-scale coherence}. The molecular dynamics constrain cellular behavior. Cellular behavior constrains tissue organization. Tissue organization constrains organ function. And crucially, the causal arrows run both ways: higher scales actively regulate lower scales. This bidirectional coupling creates a unified geometric structure spanning multiple scales.

We call this \textbf{vertical coupling}: downward regulation (coarse variables constrain fine dynamics) and upward closure (fine dynamics stabilize coarse variables), forming feedback across scales (Figure~\ref{fig:distinction}). Current AI systems are \emph{vertically flat}: they may have many layers, but higher representations do not actively maintain lower-level dynamics.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/figure1_distinction.pdf}
    \caption{\textbf{The information/geometry distinction.} Left: Information is substrate-independent and can be copied without loss (bits, facts, patterns). Right: Geometry is substrate-dependent and must be instantiated through active maintenance---coupled oscillations, cross-scale feedback, constraint closure. A database has information; a living cell has geometry.}
    \label{fig:distinction}
\end{figure}

%==============================================================================
\section{Measuring Geometry}
\label{sec:measurement}
%==============================================================================

If the distinction between simulated and instantiated geometry is real, it should be measurable. We now develop a mathematical framework for detecting genuine geometry.

\subsection{The Core Principle: Estimator Agreement}

A key insight from dynamical systems theory: in systems with coherent geometry, independent ways of measuring complexity yield consistent answers.

Consider two families of complexity estimators:
\begin{itemize}
    \item \textbf{Geometric estimators} measure state-space occupancy: correlation dimension, participation ratio \citep{gao2017theory}, intrinsic dimension.
    \item \textbf{Spectral estimators} measure oscillatory richness: spectral entropy, Fourier dimension, power spectrum scaling.
\end{itemize}

In a system with genuine geometry, these should agree. The geometric structure constrains the dynamics; the dynamics reveal the geometry. Different measurement approaches should converge on the same answer because they are probing the same underlying reality.

The novelty here is not complexity itself, but the requirement that \emph{independent estimators of complexity agree}. This distinguishes the framework from generic appeals to criticality, $1/f$ noise, or ``edge of chaos'' dynamics---all of which describe complexity without requiring cross-modal consistency.

In a system that simulates geometry, they need not agree. The simulation may be high-dimensional by one measure and low-dimensional by another, because there is no unified underlying structure enforcing consistency.

\subsection{Mathematical Foundations}

Two mathematical results make this intuition precise.

\textbf{Takens' embedding theorem} \citep{takens1981}: For deterministic systems, a time series from a single observable can reconstruct the attractor's geometry. If you measure a system's output over time and embed it in a delay-coordinate space, the resulting structure is diffeomorphic to the original attractor. Time carries geometry.

Implication: if you measure a time series from a living system, the complexity of that series places a lower bound on the dimensionality of the underlying attractor. Rich temporal dynamics require rich geometric structure. You cannot oscillate in more dimensions than you exist in.

\textbf{Dimension matching in Gaussian multiplicative chaos} \citep{lin2024}: For GMC measures in the subcritical regime ($\gamma < \sqrt{2}$), the correlation dimension $D_C$ (geometric clustering) exactly equals the Fourier dimension $D_F$ (spectral decay):
\begin{equation}
    D_C(\gamma) = D_F(\gamma) = D^*(\gamma)
\end{equation}
where:
\begin{equation}
    D^*(\gamma) = \begin{cases}
        1 - \gamma^2 & \text{if } 0 < \gamma < 1/\sqrt{2} \\
        (\sqrt{2} - \gamma)^2 & \text{if } 1/\sqrt{2} \leq \gamma < \sqrt{2}
    \end{cases}
\end{equation}

At the critical point $\gamma = \sqrt{2}$, dimension matching breaks and the measure collapses. This proves that spectral-geometric coupling is mathematically possible, enforced by cross-scale consistency, and destroyed by a sharp phase transition.

We do not claim biology literally instantiates GMC. We use it as a template: proof that estimator agreement can exist, and that it is fragile.

\subsection{Match Error: The Empirical Test}

We define \emph{match error} $\varepsilon$ to quantify estimator agreement:
\begin{equation}
    \varepsilon = \frac{|D_{\text{geom}} - D_{\text{spec}}|}{(D_{\text{geom}} + D_{\text{spec}})/2 + \delta}
    \label{eq:match_error}
\end{equation}
where $D_{\text{geom}}$ is a geometric proxy (e.g., participation ratio of delay-embedded covariance), $D_{\text{spec}}$ is a spectral proxy (e.g., normalized spectral entropy), and $\delta \approx 10^{-2}$ is a regularization constant preventing divergence when both measures are small.

\textbf{Interpretive caveats.} Match error $\varepsilon$ is an \emph{operational marker} for cross-modal consistency of constraints, not a metaphysical definition of life. It can fail innocently: finite data length, measurement noise, nonstationarity, mixed dynamical regimes, and estimator bias can all produce apparent mismatch. The claim is not that low $\varepsilon$ proves life, but that living systems should show \emph{robust} low $\varepsilon$ across temporal windows, measurement modalities, and perturbations---while simulations show fragile or architecture-dependent patterns. Systematic, reproducible estimator agreement is the signature; isolated measurements are not.

This yields two distinct failure modes:
\begin{itemize}
    \item \textbf{Pathology}: High $\varepsilon$ (estimators disagree)
    \item \textbf{Death}: Low $\varepsilon$ but trivial magnitude (both collapse toward zero)
\end{itemize}

The distinction matters: pathology is \emph{mismatch}; death is \emph{collapse}.

\textbf{Unifying three literatures.} The match error $\varepsilon$ unifies ideas from physiological complexity \citep{lipsitz1992, goldberger2002}, dynamical systems theory \citep{takens1981}, and multiscale coordination into a single operational marker. This is not a new metric so much as a recognition that these literatures have been measuring complementary aspects of the same underlying phenomenon.

\textbf{Prediction for AI}: Current AI systems should show systematic mismatch---high $D_{\text{geom}}$ (many parameters, complex representations) but low $D_{\text{spec}}$ (no intrinsic oscillations, externally clocked). This would manifest as elevated $\varepsilon$ or, more precisely, as a characteristic pattern of geometric complexity without spectral richness.

%==============================================================================
\section{Biology: The Existence Proof}
\label{sec:biology}
%==============================================================================

If geometry can be instantiated, living systems are the proof. We review evidence that biology exhibits the signature of genuine geometry: cross-scale coherence, estimator agreement, and active maintenance.

\subsection{Physiological Complexity and Mortality}

A robust finding across clinical medicine: loss of physiological complexity predicts death \citep{lipsitz1992, goldberger2002}. Healthy heart rate variability (HRV) shows rich multiscale structure---fractal scaling, $1/f$ power spectra, complex temporal patterns. As health degrades, this complexity collapses. HRV becomes periodic or random, losing the intermediate structure that characterizes viable function.

This is not merely correlation. The complexity loss \emph{precedes} clinical deterioration, suggesting it reflects a fundamental loss of physiological coordination---the collapse of instantiated geometry.

\subsection{Constraint Closure}

Living systems exhibit \textbf{constraint closure}: the system's organization constrains its dynamics, and its dynamics maintain its organization \citep{kauffman2000, moreno2015}.

A data center has massive Shannon entropy---trillions of bits---but no intrinsic organization. You can partition it arbitrarily without destroying its function. A living cell has vastly more microstate complexity, but more importantly, its organization is not arbitrary. Remove any major component and function collapses. The constraints form a closed network of mutual dependence.

Rosen \citep{rosen1991} formalized this as ``closure to efficient causation'': living systems build the machinery that builds themselves. This is not information processing in the computational sense---it is constraint-maintaining matter.

\textbf{Cross-scale coherence in practice.} Consider cardiovascular regulation: cardiac pacemaker cells generate intrinsic rhythms, modulated by autonomic innervation, which is itself regulated by baroreflex loops responding to arterial pressure, which depends on vascular tone controlled by local metabolic signals. Each scale constrains the others; none operates independently. Disrupt any level---pacemaker dysfunction, autonomic neuropathy, baroreflex failure---and the entire hierarchy destabilizes. This is geometry: a multi-scale constraint architecture that must be maintained as a whole.

\subsection{Dormancy: Geometry Without Dynamics}

Dormant states provide crucial evidence that geometry is distinct from dynamics.

Tardigrades in cryptobiosis, bacterial spores, frozen embryos, and seeds exhibit effectively negligible metabolism \citep{keilin1959, crowe2002}---drastically reduced energy flow and entropy export on biological timescales. By Schr\"{o}dinger's criterion or Prigogine's dissipative structures, they should be dead.

Yet they resume full function upon reactivation. What persists in dormancy is not dynamics but \emph{structure}: geometric relationships, constraint architecture, the shape of the attractor.

This is precisely what distinguishes viable dormancy from death. In death, the geometric structure degrades---proteins denature, membranes rupture, compartmentalization fails. The attractor dissolves. In dormancy, the structure is preserved in a stable low-energy configuration, awaiting the return of conditions that permit dynamics.

\textbf{Life is not ongoing thermodynamic activity; it is preserved geometric coherence that can support dynamics.}

This reveals a deeper distinction: geometry can be stored \emph{physically} but not \emph{symbolically}. A frozen tardigrade is not ``remembering'' its geometry---the geometry is still there, materially preserved. A paused LLM preserves structure only symbolically, as patterns in memory. Dormancy shows that biological geometry persists as physical constraint architecture, not as information \emph{about} constraints.

Crucially, more complex organisms are harder to put into viable dormancy. Mammals cannot be frozen and revived like tardigrades. This makes sense in the geometric framework: higher-dimensional systems have more degrees of freedom whose geometric relationships must be preserved. Dormancy difficulty scales with dimensionality (Figure~\ref{fig:dormancy}).

The exception proves the rule: the one mammalian cell type that \emph{can} be reliably cryopreserved is the germ cell. Sperm and oocytes carry maximal genomic information but minimal active geometry---they are information carriers awaiting instantiation, not self-maintaining constraint architectures. Germ cells did not evolve \emph{to be frozen}, but they did evolve for long-term information preservation with high fidelity: quiescent, low-dimensional, optimized for reliable transmission rather than autonomous function. Their low dimensionality makes them freezable; their high information content makes them reproductively potent. This is the information/geometry distinction in cellular form.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/figure3_dormancy.pdf}
    \caption{\textbf{Dormancy difficulty scales with dimensionality.} Probability of viable revival after cryopreservation decreases with geometric complexity. Germ cells (high information, low geometry) are readily freezable. Whole mammals (high geometry) cannot be frozen and revived. The trend reflects the increasing difficulty of preserving constraint architecture as dimensionality increases.}
    \label{fig:dormancy}
\end{figure}

\subsection{Death as Geometric Collapse}

Death is not primarily about energy depletion or specific molecular failures---these are consequences. The primary event is loss of cross-scale coordination. The geometry collapses.

A living system maintains coherent dynamics across scales: molecular reactions couple to organelle function, organelle function couples to cellular physiology, cellular physiology couples to tissue organization. Death is the decoupling of these scales. Once coordination fails at any level, the cascade propagates.

In the match error framework: death is when $D_{\text{geom}}$ and $D_{\text{spec}}$ both collapse toward zero. They may technically ``match'' (low $\varepsilon$), but only because nothing remains.

%==============================================================================
\section{Why AI Does Not Instantiate Geometry}
\label{sec:ai}
%==============================================================================

Current AI systems, including large language models, exhibit remarkable capabilities. But they do not instantiate geometry in the biological sense. We analyze why.

\subsection{Parameters Are Not Dimensions}

LLMs have trillions of parameters. This is often described as ``high-dimensional.'' But parameter count is not the same as dynamical dimensionality.

A high-dimensional parameter space means the model can represent complex functions. But during inference, the model traverses this space in a fixed way, determined by the input. There is no attractor being maintained, no dynamical manifold being defended against perturbation.

The geometry that matters for our argument is not the geometry of the weight space, but the geometry of the dynamical trajectory. And this geometry is thin: forward pass, no oscillation, no feedback from output to substrate.

\subsection{Depth Is Not Verticality}

Transformers are ``deep'' in the sense of having many layers. But depth is not verticality.

Verticality means bidirectional cross-scale coupling: higher levels actively regulate lower levels, and lower levels stabilize higher levels. In a transformer, information flows upward through layers, but there is no downward regulation of the substrate. The higher-level representations do not maintain the lower-level computations. Each forward pass starts fresh.

Living systems are vertically coupled. The organism regulates its cells, the cells regulate their organelles, the organelles regulate their molecular reactions---and the causal arrows run both ways. This creates a unified geometric structure. Transformers have layers but not levels in this sense.

\subsection{Clocked Is Not Oscillatory}

AI systems operate on external clocks. Transformers process tokens in discrete steps. Training occurs in epochs. Everything is synchronized to external timing.

Living systems generate their own time. Circadian rhythms, metabolic oscillations, calcium waves, neural oscillations---these are not imposed from outside but emerge from the system's dynamics. The oscillations are coupled to viability: disrupt them and function degrades.

This matters because oscillations are how geometry maintains itself. In a living system, the rhythms stabilize the structure, and the structure shapes the rhythms. This circular causation is the signature of constraint closure. In an AI system, there is no intrinsic rhythm to stabilize anything.

\textbf{Acknowledging dynamical AI architectures.} Recurrent neural networks, reservoir computing, spiking neural networks, and active inference agents all incorporate temporal dynamics. Our claim is not that AI lacks dynamics entirely, but that current systems lack \emph{endogenous, viability-coupled, multi-timescale dynamics that maintain the substrate itself}. A reservoir has dynamics, but they are typically driven externally and do not maintain the reservoir's own coherence. An RL agent optimizes a reward signal, but the signal is externally specified and the agent's substrate (weights, architecture) remains static during inference. The distinction is between dynamics \emph{in} computation versus dynamics \emph{of} the computing substrate.

\subsection{No Viability Coupling}

Perhaps the deepest difference: AI systems are not coupled to their own survival.

A living cell's dynamics are constrained by viability. Stray too far from the viable region of state space and you die. This creates a basin of attraction around functional states---perturbations are absorbed, damage is repaired, coherence is maintained. In living systems, loss of coherence is catastrophic \emph{to the system itself}.

An LLM has no such coupling. It does not ``care'' whether it persists. Its weights are static between training runs. There is no ongoing process working to maintain coherence. The simulation can be paused, copied, deleted---it makes no difference to the system itself. In AI systems, loss of coherence is merely an external performance issue---the substrate is unaffected.

This is not a claim about consciousness. It is a claim about dynamics. Living systems are dynamical systems whose dynamics are constrained by survival. AI systems are not.

\textbf{The ``AI is also a machine'' objection.} One might object that AI runs on physical machines too. But the distinction is not about physicality---it is about whether computation participates in substrate maintenance. In living systems, computation and substrate maintenance are inseparable: the cell's dynamics maintain the cell. In AI, computation occurs \emph{on} a machine but does not maintain it. The GPU does not care what the network computes.

\subsection{Simulation Is Impressive But Substrate-Free}

To be clear: the simulation of geometry that LLMs achieve is genuinely impressive. They learn statistical patterns that capture deep structure in concepts and their relationships. They can reason, after a fashion, by traversing these learned patterns.

But the geometry they traverse is not instantiated in the substrate. It exists as patterns in weights, activated by inputs, producing outputs. There is no underlying manifold being maintained. The ``knowing'' is simulated, not enacted.

A parallel insight has emerged in fault-tolerant quantum computation \citep{bravyi2005}. Computational power can either be \emph{injected} (prepared externally and purified, as in magic state injection) or \emph{cultivated} (grown within a protected code manifold through internal dynamics). Living systems cultivate; they maintain structure endogenously. Current AI injects; it imposes structure through training.

This connection runs deeper than analogy. When a high-dimensional system is coarse-grained---observed at a reduced resolution---the resulting description inherits structure from the underlying geometry. If the system maintains scale-invariant coherent organization, this coarse-graining yields descriptions with quantum-like properties: contextuality (measurement outcomes depend on what else is measured), non-commuting operations (the order of interventions matters), and projection-like updates under observation.\footnote{We do not claim universal macroscopic entanglement or that biological function requires long-lived quantum coherence. The claim is structural: if coherent organization is maintained across scales, the algebra of descriptions becomes quantum-like---multiple incompatible low-dimensional projections of a single high-dimensional reality---whether or not microphysical coherence survives.} This is not quantum mysticism; it is recognition that quantum formalisms naturally describe systems whose state spaces cannot be flattened into a single classical description. The mathematics of complementarity and incompatible observables may apply wherever cross-scale coherence is maintained.

%==============================================================================
\section{What Would Instantiation Require?}
\label{sec:requirements}
%==============================================================================

If current AI simulates geometry but does not instantiate it, what would instantiation require? We sketch the conditions.

\subsection{Intrinsic Oscillations}

The system would need intrinsic temporal dynamics---not externally clocked steps, but self-generated rhythms that emerge from the substrate's physics. These oscillations would need to span multiple timescales and be coupled to the system's function.

Neuromorphic architectures with genuine oscillatory dynamics point in this direction. Spiking neural networks have intrinsic temporal structure. Reservoir computing systems maintain internal dynamics. But none yet achieve the cross-scale coupling of biological oscillations.

\subsection{Viability Coupling}

The system's dynamics would need to be constrained by something like survival. This does not require consciousness or suffering---only that the system's state space has a ``viable region'' and that the dynamics tend to restore the system toward this region when perturbed.

One approach: couple the AI's operation to resource constraints that mirror metabolic limits. If computation ``costs'' something that must be replenished, and if running out of resource causes degradation, the system acquires a stake in its own persistence.

\subsection{Vertical Coupling}

Higher-level representations would need to actively regulate lower-level dynamics. This is absent in current architectures where inference is feedforward.

Recurrent architectures with top-down connections begin to address this. Predictive coding frameworks, where higher levels constrain lower-level predictions, capture some of the structure. But genuine vertical coupling would require that higher-level states maintain lower-level function---not just modulate it.

\subsection{Metabolic Expense}

Perhaps counterintuitively, instantiated geometry requires \emph{cost}. Living systems expend energy continuously to maintain their geometric coherence. This is not a bug but a feature: the metabolic expense is how the geometry gets defended against entropy.

A system that instantiates geometry would need to ``pay'' for that instantiation continuously. The relevant question is not how to make AI more efficient, but how to make it expensive in the right way---where the expense goes toward maintaining cross-scale coherence.

\subsection{The Cultivation Principle}

The unifying principle: instantiated geometry is \emph{cultivated}, not injected.

In living systems, structure is maintained from within, through ongoing dynamics constrained by viability. In current AI, structure is imposed from without, through training on external data. The cultivation principle suggests that genuine geometry requires endogenous maintenance---the system must work to preserve its own coherence.

This connects to the dormancy insight. A frozen cell retains its geometry because the structure is stable at low temperature. A paused LLM retains its weights because they are stored in memory. But only the cell's structure encodes the \emph{potential for dynamics}. The LLM's weights encode information about dynamics, not the dynamics themselves.

\subsection{The Loadability Constraint}

The analysis suggests a general principle: \textbf{information specifying a geometry can only be realized by substrates whose native dynamics already admit that class of constraints.}

You cannot ``load'' cardiac geometry into sand, or neuronal geometry into a spreadsheet, or metabolic geometry into a static database. The substrate must have the dynamical affordances---the capacity for oscillation, regulation, and repair---that the geometry requires. This is not a limitation of our technology but a constraint imposed by the nature of instantiation itself.

This loadability constraint explains why copying life is hard: not because the information is complex (though it is), but because the substrate must be capable of \emph{realizing} the constraints, not merely \emph{representing} them.

%==============================================================================
\section{Testing the Distinction}
\label{sec:testing}
%==============================================================================

The distinction between simulated and instantiated geometry generates testable predictions.

\subsection{A Minimal Pipeline}

Before detailing predictions, we specify how $\varepsilon$ could be computed from standard physiological data:

\begin{enumerate}
    \item \textbf{Data acquisition.} Obtain continuous time series (HRV R-R intervals, EEG voltage, calcium fluorescence) with sufficient length ($> 10^3$ samples) and sampling rate appropriate to the system's dominant timescales.

    \item \textbf{Preprocessing.} Detrend, artifact-reject, and segment into overlapping windows (e.g., 5-minute epochs with 50\% overlap for HRV; 2-second epochs for EEG).

    \item \textbf{Geometric proxy ($D_{\text{geom}}$).} Delay-embed each window using Takens' method (embedding dimension $m \geq 2D + 1$ where $D$ is expected attractor dimension; delay $\tau$ from first minimum of mutual information). Compute participation ratio: $\text{PR} = (\sum_i \lambda_i)^2 / \sum_i \lambda_i^2$ from eigenvalues of the embedded covariance matrix.

    \item \textbf{Spectral proxy ($R_{\text{spec}}$).} Compute power spectral density via Welch's method. Normalize to probability distribution and compute spectral entropy: $H_S = -\sum_f p(f) \log p(f)$. Convert to spectral richness by exponentiating: $R_{\text{spec}} = e^{H_S}$. This yields the effective number of frequencies contributing to the signal---a measure of spectral complexity comparable in spirit (though not in raw units) to the geometric participation ratio.

    \item \textbf{Match error.} Compute $\varepsilon = |D_{\text{geom}} - R_{\text{spec}}| / ((D_{\text{geom}} + R_{\text{spec}})/2 + \delta)$ for each window, after normalizing both proxies to comparable scales (e.g., relative to their respective system capacities). Report median and interquartile range across windows.
\end{enumerate}

This pipeline requires only standard signal processing libraries (e.g., SciPy, MATLAB Signal Processing Toolbox) and produces a single scalar $\varepsilon$ interpretable as degree of estimator agreement.

\subsection{Prediction 1: Estimator Mismatch in AI}

Current AI systems should show systematic estimator mismatch: high $D_{\text{geom}}$ (representation richness) but low $R_{\text{spec}}$ (no intrinsic oscillations, externally clocked dynamics).

\textbf{Operationalizing for AI.} For neural networks, define $D_{\text{geom}}$ on \emph{activation manifolds}: the effective rank or participation ratio of layerwise activations under a distribution of prompts. This measures how many dimensions the network actually uses, not how many parameters it has. Define $R_{\text{spec}}$ on \emph{temporal activation dynamics} in recurrent or agent settings, or on token-step variability spectra for feedforward models---noting that the latter is externally clocked and thus expected to show impoverished spectral structure.

This could be tested by measuring both geometric and spectral complexity in trained neural networks. Geometric measures (intrinsic dimension of representations, effective rank of activation matrices) should be moderate to high. Spectral measures (oscillatory structure in activation dynamics) should be low or trivial, yielding elevated $\varepsilon$ (Figure~\ref{fig:phase_space}).

\subsection{Prediction 2: Estimator Agreement in Biology}

Living systems should show estimator agreement: $D_{\text{geom}}$ and $D_{\text{spec}}$ should covary and remain mutually consistent.

This could be tested using physiological data (HRV, EEG, calcium imaging). Healthy states should show low match error $\varepsilon$. Pathological states should show elevated $\varepsilon$. Death should show collapse of both measures.

\subsection{Prediction 3: $\varepsilon$ Predicts Biological Outcomes}

If dimension matching characterizes viable life, then match error $\varepsilon$ should predict clinical outcomes.

A minimal viable experiment: use public HRV datasets with mortality outcomes. Compute geometric proxy (participation ratio of delay-embedded covariance) and spectral proxy (spectral entropy of power spectrum). Test whether $\varepsilon$ predicts mortality better than either proxy alone.

\subsection{Prediction 4: Pathology Increases $\varepsilon$ Before Collapse}

The framework predicts a temporal ordering: first $\varepsilon$ increases (estimators diverge), then both measures collapse (death).

This could be tested in longitudinal clinical data. The prediction is that rising $\varepsilon$ should precede terminal decline, providing early warning of geometric destabilization.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/figure2_phase_space.pdf}
    \caption{\textbf{Match error phase space.} Geometric complexity ($D_{\text{geom}}$) versus spectral richness ($R_{\text{spec}}$) for different system states. Healthy biological systems cluster near the diagonal (low $\varepsilon$, matched estimators). Pathological states show elevated mismatch. Death/collapse occupies the low-low corner. AI simulations occupy a distinct region: high geometric complexity (rich representations) but low spectral richness (no intrinsic oscillations), yielding systematic mismatch.}
    \label{fig:phase_space}
\end{figure}

%==============================================================================
\section{Discussion}
\label{sec:discussion}
%==============================================================================

\subsection{Implications for AI}

If the distinction between simulated and instantiated geometry is real, it suggests that scaling current architectures will not produce biological-grade intelligence. More parameters increase the complexity of the simulation, but do not instantiate the geometry.

This is not a criticism of AI's usefulness. Simulations can be extraordinarily valuable. But it suggests a ceiling on what current approaches can achieve. The ``missing ingredient'' is not more training data or larger models---it is a different kind of architecture that can instantiate and maintain geometry.

In this framework, the limitation of current AI is not lack of complexity but \emph{decoupling} between representational geometry and intrinsic dynamics. AI systems can have rich representations (high $D_{\text{geom}}$) without rich temporal structure (low $R_{\text{spec}}$). The diagnosis is lopsidedness, not deficiency.

An immediate consequence is a \textbf{no-free-lunch result for artificial life}: no amount of informational description or simulation can substitute for a substrate capable of instantiating and maintaining the required geometry. The loadability constraint is not a temporary technological limitation but a structural feature of instantiation itself.

\subsection{Implications for Biology}

The framework suggests that geometry, not information, is the signature of life. Living systems are not primarily information processors---they are geometry maintainers. The cell's job is to preserve the constrained manifold that constitutes its viability.

This reframes metabolism. Energy expenditure is not the cost of information processing but the price of geometric maintenance. Death is not running out of energy but losing the geometric coherence that energy expenditure was maintaining.

\subsection{Implications for Philosophy}

The information/geometry distinction formalizes an ancient intuition: the map is not the territory.

Information is cartography. It represents, describes, names. Geometry is geography. It constitutes, constrains, enables. A complete map of a territory is still just a map. The territory exists independently of any representation.

LLMs are extraordinary cartographers. They have mapped conceptual space to unprecedented resolution. But they remain maps. Living systems are territories---geometric structures that exist and persist independently of any description.

\subsection{Geometry as Constraint Attractor}

Recent work on ``mathematical unicorns'' \citep{janik2025} provides a complementary perspective. Some mathematical structures---the Monster group, exotic smooth structures, non-measurable sets---are not constructed but \emph{forced}: they emerge as the unique (or canonical) intersection of independently motivated constraints.

In the present framework, geometry itself has this character. A living cell's constraint architecture is not designed but forced: it is what survives the intersection of thermodynamic constraints, chemical kinetics, membrane physics, and information-theoretic limits on self-maintenance. The geometry looks arbitrary in any single projection but is necessitated by global coherence requirements.

This dissolves the puzzle of why geometry has the structure it does. The question ``why this geometry?'' becomes: ``what constraints intersect to force this attractor?'' The answer is not arbitrary, but it is not derivable from any single constraint either. Living systems occupy forced regions of possibility space---unicorns of constraint coherence.

This also explains why geometry cannot be copied as information alone. Information can specify constraints, but only substrates can instantiate their intersection. The forced structure exists only where the constraints are simultaneously satisfied---in matter that maintains the coherence, not in symbols that describe it.

\subsection{The Distinguishing Feature of Life}

We conclude with a hypothesis: \textbf{the distinguishing feature of life is not information processing but the maintenance of coherence across scales, sustained by bidirectional constraint flow.}

This coherence is what we have called geometry. It is instantiated, not simulated. It requires energy to maintain. It can persist through dormancy and collapses in death. It is what makes living matter different from dead matter, and what current AI lacks.

Intelligence, in this view, is a consequence of vertical integration. Systems that instantiate geometry across scales can integrate information across scales, coordinate responses across scales, and adapt across scales. The geometry enables the intelligence.

This suggests a direction for future AI: not more parameters but genuine oscillations; not more depth but genuine verticality; not more data but genuine viability coupling. The goal would not be to simulate geometry better, but to instantiate it at all.

Whether this is possible remains an open question. But if the analysis is correct, it is the right question to ask.

%==============================================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
