"""
Multiscale Agent Game Simulation

Demonstrates the game-theoretic interpretation of dimension matching:
- Scales as "players" in a coordination game
- Cooperative equilibrium = dimension matching (martingale balance)
- Collapse = one scale dominates (defection wins)

The model:
- N agents at different scales
- Each agent controls variance allocation at its scale
- Payoff = local gain - global instability penalty
- Nash equilibrium exists when no scale can profitably deviate
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import Tuple, List, Optional
from scipy.optimize import minimize


def multiscale_cascade(
    n_scales: int = 5,
    n_steps: int = 1000,
    allocation: Optional[np.ndarray] = None,
    noise_level: float = 0.1,
    coupling: float = 0.5
) -> np.ndarray:
    """
    Simulate a multiscale cascade with given variance allocation.

    Parameters
    ----------
    n_scales : int
        Number of scales (agents)
    n_steps : int
        Number of time steps
    allocation : ndarray, optional
        Variance allocation per scale (sums to 1). If None, uses uniform.
    noise_level : float
        Base noise level
    coupling : float
        Cross-scale coupling strength

    Returns
    -------
    signal : ndarray
        Time series generated by the cascade
    """
    if allocation is None:
        allocation = np.ones(n_scales) / n_scales

    # Normalize allocation
    allocation = allocation / np.sum(allocation)

    # Generate signal as sum of components at each scale
    signal = np.zeros(n_steps)

    for s, alloc in enumerate(allocation):
        # Scale s has characteristic frequency ~ 2^s
        freq = 2 ** s
        # Amplitude proportional to allocation
        amplitude = np.sqrt(alloc * noise_level)

        # Generate component with some randomness
        phase = np.random.uniform(0, 2*np.pi)
        t = np.arange(n_steps)

        # Mix of deterministic oscillation and noise at this scale
        component = amplitude * (
            0.5 * np.sin(2*np.pi * freq * t / n_steps + phase) +
            0.5 * np.random.randn(n_steps)
        )

        # Low-pass filter to enforce scale structure
        kernel_size = max(1, n_steps // (freq * 4))
        if kernel_size > 1:
            kernel = np.ones(kernel_size) / kernel_size
            component = np.convolve(component, kernel, mode='same')

        signal += component

    # Add cross-scale coupling using np.roll
    # The roll operation introduces phase correlations between scales without
    # requiring explicit differential equations. Rolling by 2^s samples creates
    # a delayed feedback that mimics how slower scales (larger s) influence
    # faster dynamics with a characteristic lag. This captures the essential
    # feature of multiscale coupling: information flows between scales with
    # scale-dependent delays.
    for s in range(1, n_scales):
        signal += coupling * allocation[s] * np.roll(signal, 2**s)

    return signal


def estimate_dimension_from_signal(
    signal: np.ndarray,
    method: str = 'correlation'
) -> float:
    """
    Estimate dimension from time series.

    Parameters
    ----------
    signal : ndarray
        Time series
    method : str
        'correlation' or 'spectral'

    Returns
    -------
    dim : float
        Estimated dimension
    """
    if method == 'correlation':
        # Simplified correlation dimension via embedding
        # Use time-delay embedding
        tau = 5
        m = 3  # embedding dimension
        n = len(signal)

        if n < (m-1)*tau + 100:
            return np.nan

        # Embed
        embedded = np.zeros((n - (m-1)*tau, m))
        for i in range(m):
            embedded[:, i] = signal[i*tau:n-(m-1-i)*tau]

        # Compute correlation integral for several r values
        n_pts = min(500, len(embedded))
        idx = np.random.choice(len(embedded), n_pts, replace=False)
        pts = embedded[idx]

        dists = np.sqrt(np.sum((pts[:, None, :] - pts[None, :, :])**2, axis=2))
        dists = dists[np.triu_indices(n_pts, k=1)]

        r_values = np.percentile(dists, np.linspace(5, 50, 20))
        C_values = np.array([np.mean(dists < r) for r in r_values])

        valid = C_values > 0
        if np.sum(valid) < 5:
            return np.nan

        coeffs = np.polyfit(np.log(r_values[valid]), np.log(C_values[valid]), 1)
        return coeffs[0]

    elif method == 'spectral':
        # Power spectrum decay
        ps = np.abs(np.fft.fft(signal))**2
        n = len(ps)
        freqs = np.arange(1, n//4)
        power = ps[freqs]

        valid = power > 0
        if np.sum(valid) < 5:
            return np.nan

        coeffs = np.polyfit(np.log(freqs[valid]), np.log(power[valid]), 1)
        return -coeffs[0] / 2  # Convert spectral slope to dimension-like quantity

    else:
        raise ValueError(f"Unknown method: {method}")


def compute_payoff(
    allocation: np.ndarray,
    local_gain_weight: float = 1.0,
    stability_weight: float = 2.0,
    n_samples: int = 3
) -> Tuple[float, np.ndarray]:
    """
    Compute payoff for a given variance allocation.

    Payoff = local gains - AGENT-SPECIFIC instability penalty

    The key insight: the stability penalty is agent-specific. Each agent is
    penalized for deviating from the uniform (martingale) allocation via a
    quadratic penalty: (allocation - uniform)^2. This creates a basin of
    attraction around the cooperative equilibrium. The "depth" of this basin
    (controlled by stability_weight) determines how strongly defection is
    punished. When stability_weight is high, the basin is deep and cooperation
    is stable. When low, local incentives can overcome the penalty and trigger
    collapse.

    Parameters
    ----------
    allocation : ndarray
        Variance allocation per scale
    local_gain_weight : float
        Weight for local variance gains
    stability_weight : float
        Weight for individual stability penalty (basin depth)
    n_samples : int
        Number of samples for averaging

    Returns
    -------
    total_payoff : float
        Total system payoff
    individual_payoffs : ndarray
        Payoff for each scale-agent
    """
    allocation = np.abs(allocation)
    allocation = allocation / np.sum(allocation)

    n_scales = len(allocation)
    uniform = 1.0 / n_scales

    # Local gain: each scale wants more variance (logarithmic utility)
    local_gains = local_gain_weight * np.log(allocation + 1e-10)

    # AGENT-SPECIFIC stability penalty: penalize YOUR OWN over-allocation
    # Each agent pays a cost proportional to how much they exceed uniform share
    # This quadratic penalty creates the basin of attraction around equilibrium
    individual_penalties = stability_weight * (allocation - uniform) ** 2

    # Additional penalty: agents also care about global concentration
    # (incentive to keep system from collapsing even if you're not the one dominating)
    concentration = np.sum(allocation ** 2)
    global_penalty = 0.5 * stability_weight * concentration

    # Compute dimension matching quality
    dim_match_bonus = 0
    for _ in range(n_samples):
        signal = multiscale_cascade(n_scales, allocation=allocation)
        d_corr = estimate_dimension_from_signal(signal, 'correlation')
        d_spec = estimate_dimension_from_signal(signal, 'spectral')
        if not np.isnan(d_corr) and not np.isnan(d_spec):
            dim_match_bonus += np.exp(-np.abs(d_corr - d_spec))
    dim_match_bonus /= n_samples

    # Individual payoffs: local gain - individual penalty - share of global penalty + bonus
    individual_payoffs = (local_gains
                          - individual_penalties
                          - global_penalty / n_scales
                          + dim_match_bonus / n_scales)
    total_payoff = np.sum(individual_payoffs)

    return total_payoff, individual_payoffs


def find_nash_equilibrium(
    n_scales: int = 5,
    local_gain_weight: float = 1.0,
    stability_weight: float = 2.0
) -> np.ndarray:
    """
    Find Nash equilibrium allocation using optimization.

    Parameters
    ----------
    n_scales : int
        Number of scales
    local_gain_weight : float
        Weight for local gains
    stability_weight : float
        Weight for stability

    Returns
    -------
    equilibrium : ndarray
        Nash equilibrium allocation
    """
    def neg_payoff(x):
        alloc = np.abs(x)
        alloc = alloc / np.sum(alloc)
        payoff, _ = compute_payoff(alloc, local_gain_weight, stability_weight, n_samples=3)
        return -payoff

    # Start from uniform allocation
    x0 = np.ones(n_scales) / n_scales

    result = minimize(neg_payoff, x0, method='Nelder-Mead',
                     options={'maxiter': 200, 'xatol': 0.01})

    equilibrium = np.abs(result.x)
    equilibrium = equilibrium / np.sum(equilibrium)

    return equilibrium


def simulate_dynamics(
    n_scales: int = 5,
    n_rounds: int = 50,
    learning_rate: float = 0.1,
    local_gain_weight: float = 1.0,
    stability_weight: float = 2.0,
    init_allocation: Optional[np.ndarray] = None,
    noise_scale: float = 0.01
) -> Tuple[np.ndarray, List[float], List[float]]:
    """
    Simulate learning dynamics toward equilibrium.

    Each round:
    1. Compute payoffs for current allocation
    2. Each scale adjusts allocation based on gradient

    Parameters
    ----------
    n_scales : int
        Number of scales
    n_rounds : int
        Number of learning rounds
    learning_rate : float
        Learning rate for allocation updates
    local_gain_weight : float
        Weight for local gains
    stability_weight : float
        Weight for stability
    init_allocation : ndarray, optional
        Initial allocation (if None, uses uniform + small noise)
    noise_scale : float
        Scale of noise to add to break symmetry

    Returns
    -------
    allocation_history : ndarray
        Allocation at each round
    payoff_history : list
        Total payoff at each round
    dim_match_history : list
        Dimension matching quality at each round
    """
    if init_allocation is not None:
        allocation = init_allocation.copy()
        allocation = allocation / np.sum(allocation)
    else:
        # Start near uniform but with small noise to break symmetry
        allocation = np.ones(n_scales) / n_scales
        allocation += noise_scale * np.random.randn(n_scales)
        allocation = np.clip(allocation, 0.01, 0.99)
        allocation = allocation / np.sum(allocation)

    allocation_history = [allocation.copy()]
    payoff_history = []
    dim_match_history = []

    for round_idx in range(n_rounds):
        # Compute current payoff
        payoff, individual = compute_payoff(
            allocation, local_gain_weight, stability_weight, n_samples=2
        )
        payoff_history.append(payoff)

        # Compute dimension matching
        signal = multiscale_cascade(n_scales, allocation=allocation)
        d_corr = estimate_dimension_from_signal(signal, 'correlation')
        d_spec = estimate_dimension_from_signal(signal, 'spectral')
        if not np.isnan(d_corr) and not np.isnan(d_spec):
            dim_match_history.append(np.abs(d_corr - d_spec))
        else:
            dim_match_history.append(np.nan)

        # Replicator-like update: scales with higher payoff get more
        gradient = individual - np.mean(individual)
        allocation = allocation + learning_rate * gradient * allocation
        allocation = np.clip(allocation, 0.01, 0.99)
        allocation = allocation / np.sum(allocation)

        allocation_history.append(allocation.copy())

    return np.array(allocation_history), payoff_history, dim_match_history


def plot_game_dynamics(
    n_scales: int = 5,
    stability_weights: List[float] = [0.05, 0.5, 5.0]
):
    """
    Plot game dynamics for different stability weights.

    Lower stability weight -> easier to collapse (one scale dominates)
    Higher stability weight -> dimension matching maintained

    Use extreme weights to show clearer differentiation between regimes.
    """
    fig, axes = plt.subplots(2, len(stability_weights), figsize=(14, 8))

    np.random.seed(42)  # For reproducibility

    for col, sw in enumerate(stability_weights):
        # For low stability weight, start with asymmetric allocation to trigger collapse
        if sw < 0.2:
            init_alloc = np.array([0.5, 0.2, 0.15, 0.1, 0.05])
            learning_rate = 0.3  # Faster learning amplifies instability
        elif sw < 1.0:
            init_alloc = np.array([0.3, 0.25, 0.2, 0.15, 0.1])
            learning_rate = 0.15
        else:
            # High stability: start perturbed, show RETURN to near-uniform
            # This demonstrates the basin of attraction around cooperation
            init_alloc = np.array([0.35, 0.25, 0.2, 0.12, 0.08])
            learning_rate = 0.15

        alloc_hist, payoff_hist, dim_hist = simulate_dynamics(
            n_scales=n_scales,
            n_rounds=60,
            learning_rate=learning_rate,
            stability_weight=sw,
            init_allocation=init_alloc,
            noise_scale=0.02
        )

        # Top row: allocation over time
        ax1 = axes[0, col]
        for s in range(n_scales):
            ax1.plot(alloc_hist[:, s], label=f'Scale {s+1}', linewidth=1.5)
        ax1.axhline(1/n_scales, color='k', linestyle='--', alpha=0.5, label='Martingale')
        ax1.set_xlabel('Round')
        ax1.set_ylabel('Variance Allocation')
        ax1.set_title(f'Stability Weight = {sw}')
        ax1.set_ylim(0, 0.9)
        if col == 0:
            ax1.legend(loc='upper right', fontsize=8)

        # Bottom row: dimension matching
        ax2 = axes[1, col]
        valid_dim = [d for d in dim_hist if not np.isnan(d)]
        ax2.plot(range(len(dim_hist)), dim_hist, 'b-', linewidth=2)
        ax2.axhline(0, color='g', linestyle='--', alpha=0.5, label='Perfect match')
        ax2.set_xlabel('Round')
        ax2.set_ylabel('$|D_C - D_F|$')
        max_dim = max(valid_dim) if valid_dim else 1.0
        ax2.set_ylim(0, max(2, max_dim * 1.3))
        if col == 0:
            ax2.legend()

        # Regime label
        if sw < 0.2:
            regime = 'COLLAPSE'
            color = 'red'
        elif sw > 1.5:
            regime = 'COHERENT'
            color = 'green'
        else:
            regime = 'TRANSITION'
            color = 'orange'
        ax1.text(0.5, 0.95, regime, transform=ax1.transAxes, fontsize=12,
                fontweight='bold', color=color, ha='center', va='top')

    plt.suptitle('Multiscale Coordination Game: From Collapse to Coherence',
                 fontsize=14, fontweight='bold')
    plt.tight_layout()
    plt.savefig('../figures/fig3_game_dynamics.png', dpi=150)
    plt.savefig('../figures/fig3_game_dynamics.pdf')
    print("Saved fig3_game_dynamics.png/pdf")
    plt.close()


if __name__ == "__main__":
    import os
    os.chdir(os.path.dirname(os.path.abspath(__file__)))

    print("=" * 60)
    print("Multiscale Agent Game Simulation")
    print("=" * 60)

    # Demo: compare cooperative vs collapsed allocations
    print("\n1. Comparing uniform vs concentrated allocations:")
    n_scales = 5

    # Uniform (cooperative)
    uniform_alloc = np.ones(n_scales) / n_scales
    payoff_uniform, _ = compute_payoff(uniform_alloc)
    signal_uniform = multiscale_cascade(n_scales, allocation=uniform_alloc)
    d_corr_u = estimate_dimension_from_signal(signal_uniform, 'correlation')
    d_spec_u = estimate_dimension_from_signal(signal_uniform, 'spectral')

    print(f"  Uniform allocation: payoff = {payoff_uniform:.3f}")
    print(f"    D_corr = {d_corr_u:.3f}, D_spec = {d_spec_u:.3f}, |diff| = {abs(d_corr_u-d_spec_u):.3f}")

    # Concentrated (collapsed)
    concentrated_alloc = np.array([0.8, 0.1, 0.05, 0.03, 0.02])
    payoff_conc, _ = compute_payoff(concentrated_alloc)
    signal_conc = multiscale_cascade(n_scales, allocation=concentrated_alloc)
    d_corr_c = estimate_dimension_from_signal(signal_conc, 'correlation')
    d_spec_c = estimate_dimension_from_signal(signal_conc, 'spectral')

    print(f"  Concentrated allocation: payoff = {payoff_conc:.3f}")
    print(f"    D_corr = {d_corr_c:.3f}, D_spec = {d_spec_c:.3f}, |diff| = {abs(d_corr_c-d_spec_c):.3f}")

    # Find Nash equilibrium
    print("\n2. Finding Nash equilibrium...")
    eq = find_nash_equilibrium(n_scales)
    print(f"  Equilibrium allocation: {eq}")
    payoff_eq, _ = compute_payoff(eq)
    signal_eq = multiscale_cascade(n_scales, allocation=eq)
    d_corr_e = estimate_dimension_from_signal(signal_eq, 'correlation')
    d_spec_e = estimate_dimension_from_signal(signal_eq, 'spectral')
    print(f"  Payoff = {payoff_eq:.3f}, D_corr = {d_corr_e:.3f}, D_spec = {d_spec_e:.3f}")

    # Generate dynamics plot
    print("\n3. Generating game dynamics figure...")
    plot_game_dynamics()

    print("\n" + "=" * 60)
    print("Done!")
    print("=" * 60)
